{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Processing output for 3D single person full-body tracking with FrankMocap </h1>\n",
    "\n",
    "<h3> \n",
    "    Wim Pouw ( wim.pouw@donders.ru.nl )<br>James Trujillo ( james.trujillo@donders.ru.nl )<br>\n",
    "    18-11-2021 </h3>\n",
    "    \n",
    "<img src=\"./images/envision_banner.png\"> </center>\n",
    "\n",
    "<h3> Info documents </h3>\n",
    "This module provides a demonstration of how to extract a time series from FrankMocap output. FrankMocap is a heavy-duty GPU-based single-person 3D motion tracking procedure that takes as input from 2D video and approximates 3D joint postions. So for it to run it (smoothly) you need a (strong) GPU (i.e., a good video card); see repository for specifications.\n",
    "\n",
    "FrankMocap is special in that it is dedicated for 3D motion tracking from 2D video. Where the lightweight Mediapipe body and hand tracking also provides some depth information, FrankMocap is specifically optimized for estimating as best as possible the 3D joint positions (and rotations) from 2D video for the hands and the body. See the main FrankMocap paper in the resources for performance indicators. Note that FrankMocap only allows for tracking one person, and it will actually give an error if there are more than 1 person in the frame.\n",
    "\n",
    "The installation of frankmocap can be quite cumbersome, and the procedure depends on your operating system (for a description see the FrankMocap repository linked below). We have installed frankmocap on a windows machine following the exact instructions of this helpful tutorial https://www.youtube.com/watch?v=MYLMM7jOMS4. If your institute has such resources you can ask your IT department to assist in the installation.\n",
    "\n",
    "But even when the installation is completed, the output you can then generate is not yet a time series and a tracking video that we can work with. FrankMocap generates only single video frames with the pose projected onto the frames, and per frame a .pkl file that contains tracking information. This python script therefore assists in this last step to transform the frankmocap output into workable time series data and create a video from the frames. We will extract the joint positions, but not that the script can be adapted if you want the 3D joint rotation angles instead. \n",
    "\n",
    "<h3> Executed code in conda command prompt </h3>\n",
    "We ran the full body tracking (hands and body) on the sample video to produce the motion tracking output by frankmocap.  \n",
    "\n",
    "> \"python -m demo.demo_frankmocap --input_path ./sampledata/TanDun_youtube_conductorexample.mp4 --out_dir ./mocap_output --save_pred_pkl\"\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* location code: \n",
    "https://github.com/WimPouw/EnvisionBootcamp2021/tree/main/Python/FrankMocapBodyandHandTracking\n",
    "\n",
    "* citation: \n",
    "Pouw, W.  &  Trujillo, J.P.(2021-11-18). <i> Processing output for 3D single person full-body tracking with FrankMocap </i> \\[day you visited the site]. Retrieved from: https://wimpouw.github.io/EnvisionBootcamp2021/FrankMocapBodyandHandTracking.html\n",
    "\n",
    "<h4>resources</h4>\n",
    "* FrankMocap Github Repository: https://github.com/facebookresearch/frankmocap\n",
    "<br><br>\n",
    "* Rong, Y., Shiratori, T., & Joo, H. (2020). FrankMocap: Fast monocular 3D hand and body motion capture by regression and integration. arXiv preprint arXiv:2008.08324.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #for opening pkl files\n",
    "import cv2 #for video processing functions\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#files that contains predicted points\n",
    "framefol = \"../frankmocap_output/mocap/\"\n",
    "framedata = os.listdir(framefol)\n",
    "\n",
    "#files that contain the images\n",
    "renderfol = \"../frankmocap_output/rendered/\"\n",
    "renderdata = os.listdir(renderfol)\n",
    "\n",
    "#folder for saving the rendered video to\n",
    "videofold = '../MTVideos/'\n",
    "\n",
    "#folder for saving the timeseries to\n",
    "outtsfol = '../OutputTimeseries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first render the frames that were produced by frankmocap into a motion tracking video and save it to the MTVideos folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 480\n"
     ]
    }
   ],
   "source": [
    "#check the frame properties\n",
    "images = renderdata\n",
    "checkpixels = cv2.imread(renderfol + images[0])\n",
    "height, width = checkpixels.shape[:2]\n",
    "print(width, height)\n",
    "\n",
    "# choose codec according to format needed\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V') \n",
    "video = cv2.VideoWriter(videofold +'MTvideo.mp4', fourcc, 29.97, (width, height))\n",
    "\n",
    "#loop through the images save them to a video\n",
    "for i in images:\n",
    "    img = cv2.imread(renderfol + i)\n",
    "    video.write(img)\n",
    "\n",
    "#release\n",
    "print(\"done\")\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract from the pkl files all the tracked body points. We will only do this for the upper body, as the rest was not visible. You can change the script of course to extract all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each left or right hand has\n",
    "#pred_joints_smpl\n",
    "#faces\n",
    "#bbox_scale_ratio\n",
    "#bboc_top_left\n",
    "#pred_camera\n",
    "#img_cropped\n",
    "#pred_hand_pose -> this gives the joint angles\n",
    "#pred_hand_betas \n",
    "#pred_joints_img*** -> this gives the joint positions (rescaled for camera)\n",
    "#****we will extract the rescaled joint positions\n",
    "\n",
    "#https://github.com/facebookresearch/frankmocap/issues/31\n",
    "#in body+hand mode the body points are the same as openpose\n",
    "markerspositionhand = ['Wrist', 'Thumb_00', 'Thumb_01', 'Thumb_02', 'Thumb_03', 'Index_00', 'Index_01',\n",
    "                'Index_02', 'Index_03', 'Middle_00', 'Middle_01', 'Middle_02', 'Middle_03', 'Ring_00',\n",
    "                'Ring_01', 'Ring_02', 'Ring_03', 'Little_00', 'Little_01', 'Little_02', 'Little_03']\n",
    "\n",
    "#lets make a list of the variables with x,y,z coordinates\n",
    "markerhands = []\n",
    "for i in ['L_', 'R_']:\n",
    "    for j in markerspositionhand:\n",
    "        for k in ['x_', 'y_', 'z_']:\n",
    "            markerhands.append(str.lower(k + i + j))\n",
    "\n",
    "#for upper body we only want these points and they for position they follow openpose       \n",
    "markerspositionbody = ['Nose', 'Neck', 'Rshoulder', 'Relbow', 'RWrist', 'LShoulder',\n",
    "               'LElbow', 'LWrist', 'Midhip', 'RHip', 'RKnee', 'RAnkle', 'LHip']\n",
    "              #'LKnee', 'LAnkle', 'REye', 'LEye', 'REar', 'LEar', 'LBigToe', 'LSmallToe', \n",
    "              # 'LHeel', 'RBigToe', 'RSmallToe', 'RHeel', 'Background']\n",
    "\n",
    "markerbody = []\n",
    "for j in markerspositionbody:\n",
    "    for k in ['x_', 'y_', 'z_']:\n",
    "        markerbody.append(str.lower(k + j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! Now saving to file\n",
      "done! Timeseries saved\n"
     ]
    }
   ],
   "source": [
    "columns = markerhands+markerbody #these are your column names\n",
    "\n",
    "#make a time series by concatenating row information\n",
    "timeseries = []\n",
    "timeseries.append(columns) #the first is a list with columns\n",
    "for frame in framedata:\n",
    "    with open(framefol+frame, 'rb') as f:\n",
    "        data = pickle.load(f)    \n",
    "    timsdat = data['pred_output_list']\n",
    "    timsdat = pd.DataFrame(timsdat)\n",
    "    #extract body joint positions\n",
    "    leftjoints = np.ndarray.tolist(timsdat['pred_lhand_joints_img'][0])\n",
    "    rightjoints = np.ndarray.tolist(timsdat['pred_rhand_joints_img'][0])\n",
    "    bodyjoints = np.ndarray.tolist(timsdat['pred_body_joints_img'][0])\n",
    "    #get everything in a flat list\n",
    "    lj = []\n",
    "    rj = []\n",
    "    bj = []\n",
    "    for i in leftjoints:\n",
    "        lj.extend(i)\n",
    "    for i in leftjoints:\n",
    "        rj.extend(i)\n",
    "    it = 0\n",
    "    for i in markerspositionbody:\n",
    "        bj.extend(bodyjoints[it])\n",
    "        it+=1\n",
    "    #append to the the list\n",
    "    timeseries.append(lj+rj+bj)\n",
    "\n",
    "print('done! Now saving to file')\n",
    "####################################################### data to be written row-wise in csv fil\n",
    "data = timeseries\n",
    "  \n",
    "# opening the csv file in 'w+' mode\n",
    "file = open(outtsfol + 'timeseries.csv', 'w+', newline ='')\n",
    "#write it\n",
    "with file:    \n",
    "    write = csv.writer(file)\n",
    "    write.writerows(data)\n",
    "print('done! Timeseries saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
